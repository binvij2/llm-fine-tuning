{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6SSRkUaUpKNM",
      "metadata": {
        "id": "6SSRkUaUpKNM"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login() # needs api key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6d1d27f2",
      "metadata": {
        "id": "6d1d27f2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import asyncio\n",
        "import aiofiles\n",
        "from datasets import load_dataset\n",
        "from bs4 import BeautifulSoup\n",
        "from functools import partial\n",
        "from transformers import  AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "from peft import LoraConfig, get_peft_model , prepare_model_for_kbit_training, PeftModel\n",
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "75b90f69",
      "metadata": {
        "id": "75b90f69"
      },
      "outputs": [],
      "source": [
        "# sync helper functions\n",
        "def convert_to_chatml_format(qa_pair):\n",
        "    return {\n",
        "        \"text\": f\"<|system|>\\nYou are a AI Medical Advisor and you advice on health queries</s>\\n \\\n",
        "        <|user|>\\n{qa_pair['question']}</s>\\n \\\n",
        "        <|assistant|>\\n{qa_pair['best_answer']}</s>\\n\"\n",
        "    }\n",
        "\n",
        "\n",
        "def clean_html_preserve_links(html_text):\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "\n",
        "    # Convert <a href=\"...\">link text</a> to [link text](url)\n",
        "    for a_tag in soup.find_all(\"a\", href=True):\n",
        "        link_text = a_tag.get_text(strip=True)\n",
        "        link_url = a_tag['href']\n",
        "        markdown_link = f\"[{link_text}]({link_url})\"\n",
        "        a_tag.replace_with(markdown_link)\n",
        "\n",
        "    # Remove all other tags and get text\n",
        "    return soup.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "\n",
        "# async helper functions\n",
        "async def run_in_executor(func, *args):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    return await loop.run_in_executor(None, partial(func, *args))\n",
        "\n",
        "\n",
        "async def process_post(post, sem):\n",
        "    async with sem:\n",
        "        qa_answers = post.get(\"answers\", [])\n",
        "        if not qa_answers:\n",
        "            return None\n",
        "\n",
        "        question = await run_in_executor(clean_html_preserve_links, post.get(\"question_body\", \"\"))\n",
        "\n",
        "        best = max(qa_answers, key=lambda x: x.get(\"score\", 0))\n",
        "        low = min(qa_answers, key=lambda x: x.get(\"score\", 0))\n",
        "\n",
        "        high_score = best.get(\"score\", 0)\n",
        "        lowest_score = low.get(\"score\", 0)\n",
        "\n",
        "        best_answer = await run_in_executor(clean_html_preserve_links, best.get(\"body\", \"\"))\n",
        "        lowest_answer = await run_in_executor(clean_html_preserve_links, low.get(\"body\", \"\"))\n",
        "\n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"best_answer\": best_answer,\n",
        "            \"high_score\": high_score,\n",
        "            \"lowest_answer\": lowest_answer,\n",
        "            \"lowest_score\": lowest_score\n",
        "        }\n",
        "\n",
        "async def write_jsonl_async(data, filename):\n",
        "    async with aiofiles.open(filename, mode=\"w\") as f:\n",
        "        for qa in data:\n",
        "            chatml = convert_to_chatml_format(qa)\n",
        "            await f.write(json.dumps(chatml, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "async def main(ds, output_file=\"medical_chatml.jsonl\", max_concurrent=100):\n",
        "    sem = asyncio.Semaphore(max_concurrent)\n",
        "    tasks = [process_post(post, sem) for post in ds]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    qa_set = [r for r in results if r is not None]\n",
        "    await write_jsonl_async(qa_set, output_file)\n",
        "    print(f\"Wrote {len(qa_set)} items to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d54c89f1",
      "metadata": {
        "id": "d54c89f1"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "ds = load_dataset(\"ymoslem/MedicalSciences-StackExchange\")[\"train\"]\n",
        "await main(ds, output_file=\"medical_chatml.jsonl\", max_concurrent=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8bd3f9",
      "metadata": {
        "id": "4a8bd3f9"
      },
      "outputs": [],
      "source": [
        "# Quantization Configuration\n",
        "checkpoint = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "output_dir =\"./results/peft-qlora-sml-training\"\n",
        "dataset = load_dataset(\"json\", data_files=\"medical_chatml.jsonl\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "# Load the model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.use_cache = False  # Disable cache for training\n",
        "model.config.pretraining_tp = 1  # Set pretraining tensor parallelism to 1\n",
        "# Load the LLama Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.pad_token = \"<PAD>\"\n",
        "tokenizer.padding_side = \"left\"\n",
        "  # Set pad token to eos token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94215cdb",
      "metadata": {
        "id": "94215cdb"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.0,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# Prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c-4XJmtr7h2",
      "metadata": {
        "id": "4c-4XJmtr7h2"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "        per_device_train_batch_size=10,\n",
        "        gradient_accumulation_steps=1,\n",
        "        optim=\"adamw_torch\",\n",
        "        # optim=\"paged_adamw_32bit\",\n",
        "        learning_rate=1e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        num_train_epochs=1,\n",
        "        logging_steps=10,\n",
        "        fp16=True,\n",
        "        gradient_checkpointing=False,\n",
        "        report_to=\"wandb\" # Weights & Biases logging\n",
        ")\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset['train'],\n",
        "    args=training_args,\n",
        "    peft_config=peft_config,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8AZOhFAhmW6h",
      "metadata": {
        "id": "8AZOhFAhmW6h"
      },
      "outputs": [],
      "source": [
        "trained_model_adapter_name = \"TinyLlama-1.1B-Chat-v1.0-peft-medical-sciences-adapter\"\n",
        "trained_model_merged = \"TinyLlama-1.1B-Chat-v1.0-peft-medical-sciences-merged\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e8fc1c",
      "metadata": {
        "id": "77e8fc1c"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.model.save_pretrained(trained_model_adapter_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5b074d87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b074d87",
        "outputId": "c59cd6c8-2693-4df2-db18-6487324e3f29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('TinyLlama-1.1B-Chat-v1.0-peft-medical-sciences-merged/tokenizer_config.json',\n",
              " 'TinyLlama-1.1B-Chat-v1.0-peft-medical-sciences-merged/special_tokens_map.json',\n",
              " 'TinyLlama-1.1B-Chat-v1.0-peft-medical-sciences-merged/chat_template.jinja',\n",
              " 'TinyLlama-1.1B-Chat-v1.0-peft-medical-sciences-merged/tokenizer.model',\n",
              " 'TinyLlama-1.1B-Chat-v1.0-peft-medical-sciences-merged/added_tokens.json',\n",
              " 'TinyLlama-1.1B-Chat-v1.0-peft-medical-sciences-merged/tokenizer.json')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    trained_model_adapter_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "merged_model = model.merge_and_unload()\n",
        "merged_model.save_pretrained(trained_model_merged)\n",
        "tokenizer.save_pretrained(trained_model_merged)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
